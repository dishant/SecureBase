<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>1.2.&nbsp;Network</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="performance.html" title="Chapter&nbsp;1.&nbsp;Performance Tuning"><link rel="up" href="performance.html" title="Chapter&nbsp;1.&nbsp;Performance Tuning"><link rel="prev" href="performance.html" title="Chapter&nbsp;1.&nbsp;Performance Tuning"><link rel="next" href="jvm.html" title="1.3.&nbsp;Java"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.2.&nbsp;Network</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="performance.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="jvm.html">Next</a></td></tr></table><hr></div><div class="section" title="1.2.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.network"></a>1.2.&nbsp;Network</h2></div></div></div><p>
    Perhaps the most important factor in avoiding network issues degrading Hadoop and HBbase performance is the switching hardware
    that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more). 
    </p><p>
    Important items to consider:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Switching capacity of the device</li><li class="listitem">Number of systems connected</li><li class="listitem">Uplink capacity</li></ul></div><p>
    </p><div class="section" title="1.2.1.&nbsp;Single Switch"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.1switch"></a>1.2.1.&nbsp;Single Switch</h3></div></div></div><p>The single most important factor in this configuration is that the switching capacity of the hardware is capable of 
      handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware
      can have a slower switching capacity than could be utilized by a full switch. 
      </p></div><div class="section" title="1.2.2.&nbsp;Multiple Switches"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.2switch"></a>1.2.2.&nbsp;Multiple Switches</h3></div></div></div><p>Multiple switches are a potential pitfall in the architecture.   The most common configuration of lower priced hardware is a
      simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication. 
      Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated.
      </p><p>Mitigation of this issue is fairly simple and can be accomplished in multiple ways:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Use appropriate hardware for the scale of the cluster which you're attempting to build.</li><li class="listitem">Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port</li><li class="listitem">Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth.</li></ul></div><p>
      </p></div><div class="section" title="1.2.3.&nbsp;Multiple Racks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.multirack"></a>1.2.3.&nbsp;Multiple Racks</h3></div></div></div><p>Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Poor switch capacity performance</li><li class="listitem">Insufficient uplink to another rack</li></ul></div><p>
      If the the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing 
      more of your cluster across racks.  The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks.
      The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack
      A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster. 
      </p><p>Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to
      save your ports for machines as opposed to uplinks.
      </p></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="performance.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="jvm.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Chapter&nbsp;1.&nbsp;Performance Tuning&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="performance.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.3.&nbsp;Java</td></tr></table></div></body></html>