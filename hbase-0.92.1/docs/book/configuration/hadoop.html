<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>1.3.&nbsp;Hadoop</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Configuration"><link rel="up" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Configuration"><link rel="prev" href="os.html" title="1.2.&nbsp;Operating System"><link rel="next" href="standalone_dist.html" title="1.4.&nbsp;HBase run modes: Standalone and Distributed"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.3.&nbsp;Hadoop</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="os.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="standalone_dist.html">Next</a></td></tr></table><hr></div><div class="section" title="1.3.&nbsp;Hadoop"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hadoop"></a>1.3.&nbsp;<a class="link" href="http://hadoop.apache.org" target="_top">Hadoop</a><a class="indexterm" name="d42e186"></a></h2></div></div></div><p>
              This version of HBase will only run on <a class="link" href="http://hadoop.apache.org/common/releases.html" target="_top">Hadoop
        0.20.x</a>. It will not run on hadoop 0.21.x (but may run on 0.22.x/0.23.x).
        HBase will lose data unless it is running on an HDFS that has a durable
        <code class="code">sync</code>. Hadoop 0.20.2, Hadoop 0.20.203.0, and Hadoop 0.20.204.0
	DO NOT have this attribute.
        Currently only Hadoop versions 0.20.205.x or any release in excess of this
        version has a durable sync.  You have to explicitly enable it though by
        setting <code class="varname">dfs.support.append</code> equal to true on both
        the client side -- in <code class="filename">hbase-site.xml</code> though it should
        be on in your <code class="filename">base-default.xml</code> file -- and on the
        serverside in <code class="filename">hdfs-site.xml</code> (You will have to restart
        your cluster after setting this configuration).  Ignore the chicken-little
        comment you'll find in the <code class="filename">hdfs-site.xml</code> in the
        description for this configuration; it says it is not enabled because there
        are <span class="quote">&#8220;<span class="quote">... bugs in the 'append code' and is not supported in any production
        cluster.</span>&#8221;</span> because it is not true (I'm sure there are bugs but the
        append code has been running in production at large scale deploys and is on
        by default in the offerings of hadoop by commercial vendors)
        <sup>[<a name="d42e215" href="#ftn.d42e215" class="footnote">5</a>]</sup>
    <sup>[<a name="d42e225" href="#ftn.d42e225" class="footnote">6</a>]</sup><sup>[<a name="d42e231" href="#ftn.d42e231" class="footnote">7</a>]</sup>.</p><p>Or use the
    <a class="link" href="http://www.cloudera.com/" target="_top">Cloudera</a> or
    <a class="link" href="http://www.mapr.com/" target="_top">MapR</a> distributions.
    Cloudera' <a class="link" href="http://archive.cloudera.com/docs/" target="_top">CDH3</a>
    is Apache Hadoop 0.20.x plus patches including all of the 
    <a class="link" href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/" target="_top">branch-0.20-append</a>
    additions needed to add a durable sync. Use the released, most recent version of CDH3.</p><p>
    <a class="link" href="http://www.mapr.com/" target="_top">MapR</a>
    includes a commercial, reimplementation of HDFS.
    It has a durable sync as well as some other interesting features that are not
    yet in Apache Hadoop.  Their <a class="link" href="http://www.mapr.com/products/mapr-editions/m3-edition" target="_top">M3</a>
    product is free to use and unlimited.
    </p><p>Because HBase depends on Hadoop, it bundles an instance of the
        Hadoop jar under its <code class="filename">lib</code> directory. The bundled jar is ONLY for use in standalone mode.
        In distributed mode, it is <span class="emphasis"><em>critical</em></span> that the version of Hadoop that is out
        on your cluster match what is under HBase.  Replace the hadoop jar found in the HBase
        <code class="filename">lib</code> directory with the hadoop jar you are running on
        your cluster to avoid version mismatch issues. Make sure you
        replace the jar in HBase everywhere on your cluster.  Hadoop version
        mismatch issues have various manifestations but often all looks like
        its hung up.</p><div class="section" title="1.3.1.&nbsp;Hadoop Security"><div class="titlepage"><div><div><h3 class="title"><a name="hadoop.security"></a>1.3.1.&nbsp;Hadoop Security</h3></div></div></div><p>HBase will run on any Hadoop 0.20.x that incorporates Hadoop
          security features -- e.g. Y! 0.20S or CDH3B3 -- as long as you do as
          suggested above and replace the Hadoop jar that ships with HBase
          with the secure version.</p></div><div class="section" title="1.3.2.&nbsp;dfs.datanode.max.xcievers"><div class="titlepage"><div><div><h3 class="title"><a name="dfs.datanode.max.xcievers"></a>1.3.2.&nbsp;<code class="varname">dfs.datanode.max.xcievers</code><a class="indexterm" name="d42e279"></a></h3></div></div></div><p>An Hadoop HDFS datanode has an upper bound on the number of
        files that it will serve at any one time. The upper bound parameter is
        called <code class="varname">xcievers</code> (yes, this is misspelled). Again,
        before doing any loading, make sure you have configured Hadoop's
        <code class="filename">conf/hdfs-site.xml</code> setting the
        <code class="varname">xceivers</code> value to at least the following:
        </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
      &lt;/property&gt;
      </pre><p>Be sure to restart your HDFS after making the above
        configuration.</p><p>Not having this configuration in place makes for strange looking
        failures. Eventually you'll see a complain in the datanode logs
        complaining about the xcievers exceeded, but on the run up to this one
        manifestation is complaint about missing blocks. For example:
        <code class="code">10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
        blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node:
        java.io.IOException: No live nodes contain current block. Will get new
        block locations from namenode and retry...</code>
        <sup>[<a name="d42e302" href="#ftn.d42e302" class="footnote">8</a>]</sup></p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d42e215" href="#d42e215" class="para">5</a>] </sup>Until recently only the
        <a class="link" href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/" target="_top">branch-0.20-append</a>
        branch had a working sync but no official release was ever made from this branch.
        You had to build it yourself. Michael Noll wrote a detailed blog,
        <a class="link" href="http://www.michael-noll.com/blog/2011/04/14/building-an-hadoop-0-20-x-version-for-hbase-0-90-2/" target="_top">Building
        an Hadoop 0.20.x version for HBase 0.90.2</a>, on how to build an
    Hadoop from branch-0.20-append.  Recommended.</p></div><div class="footnote"><p><sup>[<a id="ftn.d42e225" href="#d42e225" class="para">6</a>] </sup>Praveen Kumar has written
            a complimentary article,
            <a class="link" href="http://praveen.kumar.in/2011/06/20/building-hadoop-and-hbase-for-hbase-maven-application-development/" target="_top">Building Hadoop and HBase for HBase Maven application development</a>.
</p></div><div class="footnote"><code class="varname"><sup>[<a name="ftn.d42e231" href="#d42e231" class="varname">7</a>] </sup>dfs.support.append</code></div><div class="footnote"><p><sup>[<a id="ftn.d42e302" href="#d42e302" class="para">8</a>] </sup>See <a class="link" href="http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html" target="_top">Hadoop HDFS: Deceived by Xciever</a> for an informative rant on xceivering.</p></div></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="os.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="standalone_dist.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">1.2.&nbsp;Operating System&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="configuration.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.4.&nbsp;HBase run modes: Standalone and Distributed</td></tr></table></div></body></html>